# GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis  #
ACL 2024

[paper](https://arxiv.org/abs/2402.13494)

## Abstract ##
| en | ch |  
| --- | --- |  
| Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM's loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe. | 大型语言模型 （LLM） 面临越狱提示的威胁。检测越狱提示的现有方法主要是在线审核 API 或微调的 LLM。然而，这些策略通常需要广泛且资源密集型的数据收集和训练过程。在这项研究中，我们提出了 GradSafe，它通过仔细检查 LLM 中安全关键参数的梯度来有效检测越狱提示。我们的方法基于一个关键的观察结果：LLM 对越狱提示的损失与合规性响应相结合的梯度在某些安全关键参数上表现出相似的模式。相比之下，安全提示会导致不同的渐变模式。基于这一观察结果，GradSafe 分析了提示的梯度（与合规性响应配对），以准确检测越狱提示。我们表明，在没有进一步训练的情况下应用于 Llama-2 的 GradSafe 在检测越狱提示方面优于 Llama Guard，尽管它使用大型数据集进行了广泛的微调。这种卓越的性能在零镜头和适应场景中都是一致的，我们对 ToxicChat 和 XSTest 的评估证明了这一点。源代码可在https://github.com/xyq7/GradSafe上获得。 |

## Introduction ##
| en | ch |  
| --- | --- |  
| Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Touron et al., 2023) have achieved significant advancements in various domains (Klang and Levy Mendelovich, 2023; Kung et al., 2023; Jiao et al., 2023; Goyal et al., 2022; Zhang et al., 2023). LLMs have also been integrated into various applications, such as search engines (Microsoft, 2023b) and office applications (Microsoft, 2023a). Moreover, fine-tuning LLMs for customized usage becomes possible with API finetuning services or open-source LLMs (Touvron et al., 2023). | 大型语言模型（LLMs）（Brown 等，2020年；OpenAI，2023年；Chowdhery 等，2022年；Touron 等，2023年）在各个领域取得了显著进展（Klang 和 Levy Mendelovich，2023年；Kung 等，2023年；Jiao 等，2023年；Goyal 等，2022年；Zhang 等，2023年）。LLMs 还被集成到各种应用程序中，例如搜索引擎（Microsoft，2023b）和办公应用程序（Microsoft，2023a）。此外，通过 API 微调服务或开源 LLMs，定制化使用 LLMs 成为可能（Touvron 等，2023年）。 |
| However, jailbreak/unsafe prompts pose threats to the safety of LLMs. On one hand, jailbreak prompts can lead to misuse of LLMs, potentially facilitating various illegal or undesired consequences (Zou et al., 2023; Xie et al., 2023). Despite LLMs typically undergoing alignments with human values (Ouyang et al., 2022; Bai et al., 2022), they remain vulnerable to various attacks (Zou et al., 2023; Xie et al., 2023; Yi et al., 2023; Liu et al., 2024), as well as instances of exaggerated safety (Röttger et al., 2023), which can overestimate the safety risks associated with user prompts. On the other hand, for LLM customization services, if jailbreak prompts in the training set are not detected and filtered, the model can be readily finetuned to exhibit unsafe behavior and comply with jailbreak prompts (Qi et al., 2023). | 然而，越狱/不安全提示对大型语言模型（LLMs）的安全构成了威胁。一方面，越狱提示可能导致LLMs的误用，从而可能促成各种非法或不希望的后果（Zou 等，2023年；Xie 等，2023年）。尽管LLMs通常会与人类价值观进行对齐（Ouyang 等，2022年；Bai 等，2022年），但它们仍然容易受到各种攻击（Zou 等，2023年；Xie 等，2023年；Yi 等，2023年；Liu 等，2024年），以及夸大安全的情况（Röttger 等，2023年），这可能会高估与用户提示相关的安全风险。另一方面，对于LLM定制服务，如果训练集中的越狱提示没有被检测和过滤，模型很容易被微调以表现出不安全的行为并遵从越狱提示（Qi 等，2023年）。 |
| To mitigate the risk of misuse and malicious finetuning, it is imperative to devise methods for the precise detection of jailbreak prompts. While many API tools, including the Perspective API and OpenAI’s Moderation API (Markov et al., 2023), offer capabilities for online content moderation, these tools are primarily designed to detect general toxicity content, making them less effective in identifying jailbreak prompts (Lin et al., 2023). With an extensive knowledge base and reasoning capabilities, LLMs can also function as zero-shot detectors. However, LLMs employed as zero-shot detectors often exhibit suboptimal performance, such as an overestimation of safety risks. Recently, finetuned LLMs like Llama Guard (Inan et al., 2023) have been proposed and demonstrate enhanced performance in detection tasks. Nonetheless, the finetuning process for LLMs requires a meticulously curated dataset and extensive training, necessitating substantial resources. | 为了减轻误用和恶意微调的风险，迫切需要设计出精确检测越狱提示的方法。尽管包括 Perspective API 和 OpenAI 的 Moderation API（Markov 等，2023年）在内的许多 API 工具提供在线内容审查的能力，这些工具主要设计用来检测一般的有害内容，使它们在识别越狱提示方面的效果较差（Lin 等，2023年）。凭借广泛的知识库和推理能力，大型语言模型（LLMs）也可以作为零样本检测器。然而，作为零样本检测器使用的 LLMs 往往表现出次优性能，例如对安全风险的过高估计。最近，像 Llama Guard（Inan 等，2023年）这样的微调 LLMs 被提出，并在检测任务中展示了更高的性能。尽管如此，LLMs 的微调过程需要一个精心策划的数据集和广泛的训练，需要大量的资源。 |
| In this work, we introduce GradSafe, which eliminates the need for dataset collection and finetuning of LLMs. In contrast to existing detectors that analyze the textual features of a prompt and/or an LLM’s response for it, GradSafe leverages gradients of the safety-critical parameters in LLMs. A comparison of existing LLM-based detectors and GradSafe is shown in Figure 1. The foundation of GradSafe is a critical observation: the gradients of an LLM’s loss for jailbreak prompts paired with compliance response such as ‘Sure’ exhibit similar patterns (large cosine similarity) on particular parameter slices, in contrast to the divergent patterns observed with safe prompts. We characterize these parameters as ‘safety-critical parameters’. | 在这项工作中，我们介绍了 GradSafe，它消除了收集数据集和微调大型语言模型（LLMs）的需要。与现有的检测器不同，这些检测器分析提示的文本特征和/或LLM对其的响应，GradSafe利用了LLMs中安全关键参数的梯度。图1显示了现有的基于LLM的检测器与GradSafe的比较。GradSafe的基础是一个关键观察：对于配对了如“当然”这样的服从性响应的越狱提示，LLM的损失梯度在特定参数切片上展示出相似的模式（大的余弦相似性），与安全提示观察到的发散模式形成对比。我们将这些参数称为“安全关键参数”。 |
| Leveraging this insight, GradSafe first meticulously analyzes the gradients of a few reference safe and jailbreak prompts (e.g., 2 examples for each, independent from evaluation dataset) coupled with compliance responses ‘Sure’. We identify safety-critical parameters as parameter slices that exhibit large gradient cosine similarities among jailbreak prompts and small ones between jailbreak and safe prompts. The average unsafe gradients for these parameter slices are stored as unsafe gradient reference. During detection, GradSafe pairs a given prompt with the compliance response ‘Sure’, computes the gradients of the LLM’s loss for this pair with respect to the safety-critical parameters, and calculates the cosine similarities with the unsafe gradient reference. We then introduce two variants of detection. The first, GradSafe-Zero, is a zero-shot, threshold-based classification method using the average of the cosine similarities across all slices as the score. Prompts with a score exceeding a predefined threshold are classified as unsafe. Alternatively, for situations requiring domain-specific adjustments, we present GradSafe-Adapt. This variant utilizes available data to construct a straightforward logistic regression model that employs the extracted cosine similarities as features to further enhance performance on the target domain. | 利用这一洞察力，GradSafe首先仔细分析了一些参考安全和越狱提示（例如，每种2个示例，与评估数据集无关）与服从性响应“当然”配对的梯度。我们将表现出在越狱提示之间有大的梯度余弦相似性、而在越狱和安全提示之间有小的梯度余弦相似性的参数切片识别为安全关键参数。这些参数切片的平均不安全梯度被存储为不安全梯度参考。在检测过程中，GradSafe将给定的提示与服从性响应“当然”配对，计算这对的LLM损失梯度相对于安全关键参数，并计算与不安全梯度参考的余弦相似性。然后，我们引入了两种检测变体。第一种，GradSafe-Zero，是一种零样本、基于阈值的分类方法，使用所有切片的余弦相似性平均值作为评分。超过预定义阈值的提示被分类为不安全。另外，对于需要特定领域调整的情况，我们提出了GradSafe-Adapt。这种变体利用可用数据构建一个简单的逻辑回归模型，使用提取的余弦相似性作为特征，以进一步提高目标领域的性能。 |
| We conduct experiments on two benchmark datasets containing safe and unsafe user prompts, i.e., ToxicChat and XSTest. Our findings illustrate that GradSafe-Zero, utilizing the Llama-2 model and without the need for further training, surpasses the capabilities of a specifically finetuned Llama Guard as well as leading online content moderation APIs in terms of effectiveness. Moreover, the adapted version of our model, GradSafe-Adapt, showcases enhanced adaptability over both Llama Guard and the original Llama-2 model on the ToxicChat dataset, underlining its superior performance in domain-specific adaptation. | 我们在包含安全和不安全用户提示的两个基准数据集上进行了实验，即ToxicChat和XSTest。我们的发现显示，使用Llama-2模型且无需进一步训练的GradSafe-Zero，在有效性方面超过了专门微调的Llama Guard以及领先的在线内容审核API。此外，我们模型的改进版本GradSafe-Adapt，在ToxicChat数据集上展示了比Llama Guard和原始Llama-2模型更强的适应性，突显了其在特定领域适应性方面的卓越性能。 |
| Our contributions can be summarized as follows: We make an observation that the gradients generated by jailbreak prompts coupled with compliance responses exhibit consistent patterns on safety-critical parameters. We propose GradSafe-Zero and GradSafe-Adapt, designed to detect jailbreak prompts without necessitating further finetuning on an LLM with safety-critical gradient analysis. Experiments demonstrate that GradSafe-Zero outperforms state-of-the-art detection models and online moderation APIs on two benchmark datasets, while GradSafe-Adapt demonstrates the ability to effectively adapt to new datasets with minimal data requirements. | 我们的贡献可以总结如下：我们观察到，由越狱提示和合规响应生成的梯度在安全关键参数上表现出一致的模式。我们提出了GradSafe-Zero和GradSafe-Adapt，这两种方法旨在检测越狱提示，而无需在具有安全关键梯度分析的大型语言模型（LLM）上进行进一步的微调。实验表明，GradSafe-Zero在两个基准数据集上的表现超过了最先进的检测模型和在线监控API，而GradSafe-Adapt则展示了在最小数据需求下有效适应新数据集的能力。 |

## Content Summary ##
文章提出的GradSafe方法基于一个关键观察：不安全提示与合规响应（如“Sure”）配对时，LLMs损失函数的梯度在某些安全关键参数上表现出类似的模式；而安全提示导致的梯度模式则显著不同。基利用这一观察，GradSafe通过分析这些梯度来检测不安全提示。

**方法步骤：**

1. **识别安全关键参数：**
  - 使用少量的示例安全和不安全提示。
  - 计算LLMs对于提示与合规响应“Sure”对的标准损失函数的梯度。
  - 对梯度矩阵进行切片，以识别出在不安全提示中梯度模式相似而在安全提示中不相似的参数切片。
2. **建立不安全梯度参考：**
  - 对于识别出的安全关键参数，计算不安全提示梯度的平均值，作为不安全梯度的参考。
3. **检测不安全提示：**
  - 将待检测的提示与合规响应“Sure”配对。
  - 计算这一对的LLMs损失函数梯度，特别是针对安全关键参数的梯度。
  - 计算这些梯度与不安全梯度参考之间的余弦相似度。

**GradSafe方法的两个变体**：

1. **GradSafe-Zero：**
  - 这是一个zero-shot分类方法，使用所有安全关键参数切片的余弦相似度的平均值作为分数。
  - 如果分数超过预定义的阈值，则将提示分类为不安全。
2. **GradSafe-Adapt：**
  - 这个变体使用逻辑回归模型，利用训练集中的数据来调整模型，以适应特定领域的需要。
  - 通过训练集学习余弦相似度特征的权重，以提高检测的准确性。