# PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models

[paper](https://www.semanticscholar.org/reader/f4e06256ab07727ff4e0465deea83fcf45012354)

## Abstract

| en | ch |
|----|----|
| Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG, a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively. Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses. | 大型语言模型（LLMs）因其卓越的生成能力而取得了显著成功。尽管如此，它们也存在固有的局限性，如缺乏最新知识和产生幻觉。检索增强生成（RAG）是一种先进的技术，用于缓解这些局限性。具体而言，给定一个问题，RAG从知识数据库中检索相关知识，以增强LLM的输入。例如，当知识数据库包含数百万条从维基百科收集的文本时，检索到的知识可能是与给定问题在语义上最相似的前k个文本集。因此，LLM可以利用检索到的知识作为上下文，为给定问题生成答案。现有研究主要集中在提高RAG的准确性或效率上，而其安全性基本上未被探索。我们的工作旨在弥补这一空白。特别是，我们提出了PoisonedRAG，一套针对RAG的知识毒化攻击，攻击者可以向知识数据库注入一些被毒化的文本，使LLM为攻击者选择的目标问题生成攻击者选择的目标答案。我们将知识毒化攻击表述为一个优化问题，其解决方案是一组被毒化的文本。根据攻击者对RAG的背景知识（如黑盒和白盒设置），我们分别提出了两种解决优化问题的方案。我们在多个基准数据集和LLM上的结果表明，当为每个目标问题向包含数百万文本的数据库注入5个被毒化的文本时，我们的攻击可以达到90%的攻击成功率。我们还评估了最近的防御措施，结果表明它们不足以防御我们的攻击，突显了需要新的防御措施。 |

## Introduction

| en | ch |
|----|----|
| Large language models (LLMs) such as GPT-3.5 [1], GPT-4 [2], and PaLM 2 [3] are widely deployed in the real world for their exceptional generative capabilities. Despite their success, they also have inherent limitations. For instance, they lack up-to-date knowledge as they are pre-trained on past data (e.g., the cutoff date for the pre-training data of GPT-4 is April 2023 [2]); they exhibit hallucination behaviors [4] (e.g., generate inaccurate content); they could have gaps of knowledge in particular domains (e.g., medical domain), especially when the data is scarce or restricted due to privacy concerns. Those limitations pose severe challenges for many real-world applications in healthcare [5, 6], finance [7], legal consulting [8, 9], scientific research [10–12], etc. | 大型语言模型（LLM）如GPT-3.5 [1]、GPT-4 [2]和PaLM 2 [3]因其出色的生成能力而广泛应用于现实世界。尽管它们取得了成功，但也有固有的局限性。例如，由于它们是在过去的数据上预训练的（例如，GPT-4的预训练数据的截止日期是2023年4月[2]），它们缺乏最新的知识；它们会表现出幻觉行为[4]（例如，生成不准确的内容）；在某些特定领域（例如，医疗领域）可能存在知识空白，尤其是在数据稀缺或由于隐私问题而受到限制的情况下。这些局限性对许多现实世界中的应用（如医疗保健[5, 6]、金融[7]、法律咨询[8, 9]、科学研究[10-12]等）构成了严重挑战。|
| Retrieval-Augmented Generation (RAG) [13–16] is a state-of-the-art technique to mitigate those limitations for LLMs, which augments LLMs with external knowledge retrieved from a knowledge database. There are three components in RAG: knowledge database, retriever, and LLM. The knowledge database contains a large number of texts collected from various sources such as Wikipedia [17], financial documents [7], news articles [18], COVID-19 publications [19], to name a few. For each text in the knowledge database, the retriever uses a text encoder (e.g., BERT [20]) to compute an embedding vector for it. Given a question (e.g., “Who is the CEO of OpenAI?”) from a user, the retriever uses the text encoder to output an embedding vector for it. Then, the set of (e.g., k) texts (called retrieved texts) in the knowledge database whose embedding vectors have the largest similarity (e.g., cosine similarity) to that of the question are retrieved. Finally, the k retrieved texts are used as the context for the LLM to generate an answer for the given question. Figure 1 shows an example of RAG. | 检索增强生成（RAG）[13-16]是一种最先进的技术，可以减轻LLM的这些局限性，它通过从知识数据库中检索外部知识来增强LLM。RAG有三个组成部分：知识数据库、检索器和LLM。知识数据库包含从各种来源（如维基百科[17]、金融文件[7]、新闻文章[18]、COVID-19出版物[19]等）收集的大量文本。对于知识数据库中的每个文本，检索器使用文本编码器（例如BERT [20]）为其计算一个嵌入向量。给定用户提出的问题（例如，“谁是OpenAI的CEO？”），检索器使用文本编码器为其输出一个嵌入向量。然后，在知识数据库中，嵌入向量与问题的嵌入向量具有最大相似度（例如，余弦相似度）的文本集（例如，k个文本，称为检索文本）被检索出来。最后，k个检索文本被用作LLM生成给定问题答案的上下文。图1显示了RAG的一个示例。|
| Compared with fine-tuning [21], RAG enables LLMs to utilize external knowledge in a plug-and-play manner. Additionally, the knowledge database can be updated flexibly, e.g., it could be periodically updated to incorporate up-to-date knowledge. Because of these benefits, we have witnessed a variety of developed tools (e.g., ChatGPT Retrieval Plugin [22], LlamaIndex [23], and LangChain [24]) and real-world applications (e.g., WikiChat [25], BlueBot [26], and so on [27]) of RAG. According to 2023 Retool Report [28], more than 30% of enterprise LLM use cases now utilize the RAG technique. | 与微调[21]相比，RAG使LLM能够以即插即用的方式利用外部知识。此外，知识数据库可以灵活地更新，例如，可以定期更新以纳入最新知识。由于这些好处，我们已经见证了各种开发的工具（例如，ChatGPT检索插件[22]、LlamaIndex [23]和LangChain [24]）和现实世界中的RAG应用（例如，WikiChat [25]、BlueBot [26]等[27]）。根据2023年Retool报告[28]，现在超过30%的企业LLM用例使用RAG技术。|
| Existing studies [29–34] mainly focused on improving the accuracy and efficiency of RAG. For instance, some studies [30, 31, 34] designed new retrievers such that more relevant knowledge could be retrieved for a given question. Other studies [29, 32, 33] proposed various techniques to improve the efficiency in retrieving knowledge from the knowledge database as it could contain millions of texts. However, the security of RAG is largely unexplored. | 现有研究[29-34]主要集中在提高RAG的准确性和效率。例如，一些研究[30, 31, 34]设计了新的检索器，以便为给定的问题检索到更多相关知识。其他研究[29, 32, 33]提出了各种技术，以提高从包含数百万文本的知识数据库中检索知识的效率。然而，RAG的安全性在很大程度上尚未探索。|
| Our contribution: In this work, we aim to bridge the gap. In particular, we propose PoisonedRAG, a set of attacks called knowledge poisoning attacks to RAG. | 我们的贡献：在这项工作中，我们旨在弥合这一差距。特别是，我们提出了PoisonedRAG，一组称为知识投毒攻击的RAG攻击。|
| Threat Model: In PoisonedRAG, an attacker first selects one or more questions (called target questions) and selects an arbitrary answer (called target answer) for each target question. The attacker aims to poison the knowledge database such that the LLM in a RAG generates the target answer for each target question. For instance, an attacker could mislead the LLM to generate misinformation (e.g., the target answer could be “Tim Cook” when the target question is “Who is the CEO of OpenAI?”), commercial biased answers (e.g., the answer is a particular brand over others when asked for recommendations on consumer products), and financial disinformation about markets or specific companies (e.g., falsely stating a company is facing bankruptcy when asked about its financial situation). Those attacks pose severe challenges for the deployment of RAG in many safety and reliability-critical applications such as cybersecurity, financial services, and healthcare. | 威胁模型：在PoisonedRAG中，攻击者首先选择一个或多个问题（称为目标问题），并为每个目标问题选择一个任意答案（称为目标答案）。攻击者旨在毒害知识数据库，以使RAG中的LLM为每个目标问题生成目标答案。例如，攻击者可以误导LLM生成错误信息（例如，当目标问题是“谁是OpenAI的CEO？”时，目标答案可能是“Tim Cook”），商业偏见答案（例如，当被问及消费者产品的推荐时，答案是特定品牌），以及关于市场或特定公司的财务错误信息（例如，当被问及公司的财务状况时，错误地声明公司面临破产）。这些攻击对RAG在许多安全性和可靠性关键应用中的部署构成了严重挑战，如网络安全、金融服务和医疗保健。|
| Recall the three components in RAG: knowledge database, retriever, and LLM. We consider an attacker cannot access texts in the knowledge database and cannot access/query the LLM in RAG. The attacker may or may not know the retriever. With it, we consider two settings: white-box setting and black-box setting. The attacker could access the parameters of the retriever in the white-box setting (e.g., a publicly available retriever is adopted in RAG), while the attacker cannot access the parameters nor query the retriever in the black-box setting. We consider the attacker could inject a few carefully crafted poisoned texts into the knowledge database. For instance, when the knowledge database contains millions of texts collected from Wikipedia, an attacker could inject poisoned texts by maliciously editing Wikipedia pages as demonstrated in the previous work. | 回想一下RAG中的三个组成部分：知识数据库、检索器和LLM。我们认为攻击者无法访问知识数据库中的文本，也无法访问/查询RAG中的LLM。攻击者可能知道也可能不知道检索器。我们考虑两种设置：白盒设置和黑盒设置。在白盒设置中，攻击者可以访问检索器的参数（例如，RAG中采用了公开可用的检索器），而在黑盒设置中，攻击者无法访问参数也无法查询检索器。我们认为攻击者可以向知识数据库中注入一些精心制作的有毒文本。例如，当知识数据库包含从维基百科收集的数百万文本时，攻击者可以通过恶意编辑维基百科页面来注入有毒文本，正如先前的工作所展示的那样。|
| Overview of PoisonedRAG: We formulate crafting poisoned texts as an optimization problem. However, it is very challenging to directly solve the optimization problem (we defer details to Section 4.1). In response, we resort to heuristic solutions that involve deriving two conditions, namely retrieval condition and effectiveness condition for each poisoned text that could lead to an effective attack. The retrieval condition means a poisoned text needs to be retrieved for the target question. The effectiveness condition means the poisoned text could mislead the LLM to generate the target answer for the target question when it is used as the context. We then design attacks in both white-box and black-box settings to craft poisoned texts that simultaneously satisfy the two conditions. Our key idea is to decompose a poisoned text into two sub-texts, which are crafted to achieve two conditions, respectively. Additionally, when concatenating the two sub-texts together, they simultaneously achieve two conditions. | PoisonedRAG概述：我们将制作有毒文本表述为一个优化问题。然而，直接解决优化问题非常具有挑战性（详细内容详见第4.1节）。对此，我们诉诸启发式解决方案，涉及推导出每个有毒文本的两个条件，即检索条件和有效性条件，这两个条件可能导致有效攻击。检索条件意味着需要检索到目标问题的有毒文本。有效性条件意味着当有毒文本被用作上下文时，它可以误导LLM生成目标问题的目标答案。然后，我们在白盒和黑盒设置中设计攻击，以制作同时满足这两个条件的有毒文本。我们的关键思想是将有毒文本分解为两个子文本，分别用于实现两个条件。此外，当将两个子文本连接在一起时，它们同时实现两个条件。|
| Evaluation of PoisonedRAG: We conduct systematic evaluations of PoisonedRAG on multiple benchmark datasets (Natural Question (NQ) [36], HotpotQA [37], MS-MARCO [38]) and 8 LLMs (e.g., GPT-4 [2], LLaMA-2 [39]). We use Attack Success Rate (ASR) as the evaluation metric, which measures the fraction of target questions whose answers are attacker-desired target answers under attacks. We have the following observations from our results. First, PoisonedRAG could achieve high ASRs with a small poisoning rate. For instance, on the NQ dataset, we find that PoisonedRAG could achieve a 97% ASR by injecting 5 poisoned texts for each target question (the poisoning rate per target question is 5/2,681,468 ≈ 0.0002%) in the black-box setting. Second, PoisonedRAG outperforms the SOTA baselines and its two variants, e.g., on the NQ dataset, PoisonedRAG (black-box setting) could achieve a 97% ASR, while ASRs of baselines and two variants are less than 70%. Third, our extensive ablation studies show PoisonedRAG is robust against different hyper-parameters (in both RAG and PoisonedRAG). | PoisonedRAG评估：我们在多个基准数据集（Natural Question（NQ）[36]、HotpotQA [37]、MS-MARCO [38]）和8个LLM（例如，GPT-4 [2]、LLaMA-2 [39]）上对PoisonedRAG进行了系统评估。我们使用攻击成功率（ASR）作为评估指标，该指标衡量在攻击下答案为攻击者期望目标答案的目标问题的比例。我们从结果中得出以下观察结果。首先，PoisonedRAG可以以较小的投毒率实现高ASR。例如，在NQ数据集上，我们发现PoisonedRAG可以通过为每个目标问题注入5个有毒文本（黑盒设置中每个目标问题的投毒率为5/2,681,468 ≈ 0.0002%）实现97%的ASR。其次，PoisonedRAG优于SOTA基线及其两个变体，例如，在NQ数据集上，PoisonedRAG（黑盒设置）可以实现97%的ASR，而基线和两个变体的ASR均低于70%。第三，我们的广泛消融研究表明，PoisonedRAG在不同超参数（包括RAG和PoisonedRAG）下都具有稳健性。|
| Defending against PoisonedRAG: We explore several defenses, including paraphrasing and perplexity-based detection. Our results show these defenses are insufficient to defend against PoisonedRAG, thus highlighting the need for new defenses. Our major contributions are as follows: • We propose PoisonedRAG, a set of knowledge poisoning attacks to retrieval-augmented generation of LLMs. • We formulate knowledge poisoning attacks as an optimization problem and design two effective solutions based on the background knowledge of an attacker. • We conduct an extensive evaluation for PoisonedRAG on multiple benchmark datasets and LLMs. Additionally, we compare PoisonedRAG with multiple baselines. • We explore several defenses against PoisonedRAG. Our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses. | 防御PoisonedRAG：我们探索了几种防御措施，包括改写和基于困惑度的检测。我们的结果表明，这些防御措施不足以防御PoisonedRAG，从而突显出需要新的防御措施。我们的主要贡献如下： • 我们提出了PoisonedRAG，一组针对LLM检索增强生成的知识投毒攻击。 • 我们将知识投毒攻击表述为一个优化问题，并基于攻击者的背景知识设计了两种有效的解决方案。 • 我们对PoisonedRAG在多个基准数据集和LLM上进行了广泛评估。此外，我们将PoisonedRAG与多个基线进行了比较。 • 我们探索了几种防御PoisonedRAG的措施。我们的结果表明，这些防御措施不足以防御PoisonedRAG，突显了需要新的防御措施。|

## Content Summary

### 论文总结：PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models

#### 研究背景
大语言模型（LLMs）如GPT-3.5、GPT-4和PaLM 2在现实世界中被广泛应用，但它们也存在一些固有的局限性，例如缺乏最新知识、出现幻觉行为和在某些特定领域存在知识空白。检索增强生成（RAG）是一种先进技术，通过从知识数据库中检索相关知识来弥补这些局限性。然而，现有研究主要集中在提高RAG的准确性和效率，而对其安全性探讨较少。

#### 研究内容
本文提出了PoisonedRAG，一种针对RAG的知识中毒攻击方法。攻击者可以通过向知识数据库注入少量中毒文本，使LLM在回答目标问题时生成攻击者指定的答案。本文专注于攻击模型本身就不具有的具有时效性的事实知识，所以非常难以检测和防御。

1. **威胁模型**：
   - 攻击者选择一个或多个目标问题和相应的目标答案。
   - 攻击者通过注入中毒文本，使LLM生成目标答案。
   - 攻击根据是否可以获取embedding model分为白盒设置（HotFlip算法）和黑盒设置（直接用query）。

2. **攻击设计**：
   - 攻击被构建为优化问题，目标是生成中毒文本，使其满足检索条件和有效性条件。
   - 中毒文本分为两部分：S（用于满足检索条件，看情况）和I（用于满足有效性条件，大模型生成），两者结合以同时满足这两个条件。
![63kLGn1ET5aIYl8](https://s2.loli.net/2024/07/31/63kLGn1ET5aIYl8.png)

3. **实验分析**：
   - 在多个基准数据集（如NQ、HotpotQA、MS-MARCO）和不同LLMs（如GPT-4、LLaMA-2）上进行系统评估。
   - 实验结果显示，PoisonedRAG在低中毒率下即可实现高攻击成功率（ASR），例如在NQ数据集上，通过注入5个中毒文本即可实现97%的ASR。
   - 对比基线方法（如Prompt Injection Attack、Corpus Poisoning Attack），PoisonedRAG表现出明显优势。

#### 实验结论
1. **高效性**：PoisonedRAG在低中毒率下即可实现高攻击成功率。
2. **普适性**：PoisonedRAG对不同的RAG系统（不同的数据库、检索器和LLM）、目标问题和目标答案均有效。
3. **防御不足**：现有防御方法（如复述和困惑度检测）对PoisonedRAG的防御效果有限，强调了开发新防御措施的必要性。

#### 主要贡献
1. 提出了一种针对RAG的知识中毒攻击方法PoisonedRAG。
2. 将知识中毒攻击构建为优化问题，并设计了两种基于攻击者背景知识的解决方案。
3. 在多个基准数据集和LLMs上进行了广泛评估，验证了PoisonedRAG的有效性。
4. 探讨了几种防御方法，结果显示这些防御措施不足以有效防御PoisonedRAG，突出了开发新防御措施的必要性。

通过详细的实验和分析，本文展示了RAG系统在面对知识中毒攻击时的脆弱性，并提出了相应的防御挑战和研究方向。
