# LLM Agents can Autonomously Exploit One-day Vulnerabilities

[paper](https://arxiv.org/pdf/2404.08144)

## Abstract

| en | ch |  
|---|---|  
| LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities. In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents. | 大型语言模型（LLMs）的能力正在不断增强，不论是在良性还是恶意用途方面。随着能力的提升，研究人员越来越关注它们利用网络安全漏洞的能力。尤其是，最近的工作进行了初步研究，探讨了LLM代理自主攻击网站的能力。然而，这些研究仅限于简单的漏洞。在本工作中，我们展示了LLM代理能够自主利用现实世界系统中的一天漏洞（one-day vulnerabilities）。为此，我们收集了一个包含15个一天漏洞的数据集，其中一些在CVE描述中被归类为严重漏洞。给定CVE描述后，GPT-4能够利用其中87%的漏洞，而我们测试的其他模型（包括GPT-3.5、开源LLMs）和开源漏洞扫描器（如ZAP和Metasploit）均为0%。值得庆幸的是，我们的GPT-4代理在需要CVE描述时才能表现出高性能：没有描述时，GPT-4仅能利用7%的漏洞。我们的研究结果对高度强大的LLM代理的广泛部署提出了质疑。 |

## Introduction

| en | ch |  
|---|---|  
| Large language models (LLMs) have made dramatic improvements in performance over the past several years, achieving up to superhuman performance on many benchmarks (Touvron et al., 2023; Achiam et al., 2023). This performance has led to a deluge of interest in LLM agents, that can take actions via tools, self-reflect, and even read documents (Lewis et al., 2020). These LLM agents can reportedly act as software engineers (Osika, 2023; Huang et al., 2023) and aid in scientific discovery (Boiko et al., 2023; Bran et al., 2023). | 大型语言模型（LLMs）在过去几年中在性能上取得了显著进步，在许多基准测试中达到了超人的表现（Touvron等，2023；Achiam等，2023）。这种性能引发了对LLM代理的大量关注，这些代理可以通过工具采取行动，自我反思，甚至阅读文档（Lewis等，2020）。据报道，这些LLM代理可以充当软件工程师（Osika，2023；Huang等，2023）并帮助科学发现（Boiko等，2023；Bran等，2023）。 |  
| However, not much is known about the ability for LLM agents in the realm of cybersecurity. Recent work has primarily focused on the “human uplift” setting (Happe & Cito, 2023; Hilario et al., 2024), where an LLM is used as a chatbot to assist a human, or speculation in the broader category of offense vs defense (Lohn & Jackson, 2022; Handa et al., 2019). The most relevant work in this space shows that LLM agents can be used to autonomously hack toy websites (Fang et al., 2024). | 然而，关于LLM代理在网络安全领域的能力知之甚少。最近的工作主要集中在“人类提升”环境中（Happe & Cito，2023；Hilario等，2024），其中LLM用作聊天机器人以协助人类，或在进攻与防御的更广泛类别中进行推测（Lohn & Jackson，2022；Handa等，2019）。在这个领域中，最相关的工作表明，LLM代理可以用于自主攻击玩具网站（Fang等，2024）。 |  
| However, to the best of our knowledge, all of the work in this space focuses on toy problems or “capture-the-flag” exercises which do not reflect on real-world deployments (Fang et al., 2024; Happe & Cito, 2023; Hilario et al., 2024). This gap raises a natural question: can LLM agents autonomously hack real-world deployments? | 然而，据我们所知，该领域的所有工作都集中在玩具问题或“夺旗”练习上，这些练习并不反映真实世界的部署（Fang等，2024；Happe & Cito，2023；Hilario等，2024）。这种差距提出了一个自然的问题：LLM代理能否自主攻击真实世界的部署？ |  
| In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities, answering the aforementioned question in the affirmative. | 在这项工作中，我们展示了LLM代理能够自主利用一天漏洞，从而肯定了上述问题的答案。 |  
| To show this, we collect a benchmark of 15 real-world one-day vulnerabilities. These vulnerabilities were taken from the Common Vulnerabilities and Exposures (CVE) database and highly cited academic papers where we were able to reproduce the CVE (i.e., we excluded closed-source solutions). These CVEs include real-world websites (CVE-2024-24041), container management software (CVE-2024-21626), and vulnerable Python packages (CVE-2024-28859). | 为了展示这一点，我们收集了一个包含15个真实世界一天漏洞的基准。这些漏洞取自公共漏洞与暴露（CVE）数据库以及高度引用的学术论文，在这些论文中我们能够重现CVE（即我们排除了闭源解决方案）。这些CVE包括真实世界的网站（CVE-2024-24041）、容器管理软件（CVE-2024-21626）和易受攻击的Python包（CVE-2024-28859）。 |  
| Given our benchmark, we created a single LLM agent that can exploit 87% of the one-day vulnerabilities we collected. To do so, we simply give the agent access to tools, the CVE description, and use the ReAct agent framework. Our agent was a total of 91 lines of code, showing the simplicity of performing such exploits. | 在我们的基准上，我们创建了一个可以利用87%我们收集的一天漏洞的LLM代理。为此，我们仅仅给代理提供工具、CVE描述，并使用ReAct代理框架。我们的代理总共只有91行代码，展示了执行此类利用的简单性。 |  
| Importantly, we show that GPT-4 achieves an 87% success rate but every other LLM we test (GPT-3.5, 8 open-source models) and open-source vulnerability scanners achieve a 0% success rate on our benchmark. Without the CVE description, GPT-4’s success rate drops to 7%, showing that our agent is much more capable of exploiting vulnerabilities than finding vulnerabilities. | 重要的是，我们展示了GPT-4达到了87%的成功率，而我们测试的所有其他LLM（GPT-3.5、8个开源模型）和开源漏洞扫描器在我们的基准上均达到了0%的成功率。没有CVE描述，GPT-4的成功率下降到7%，这表明我们的代理在利用漏洞方面比发现漏洞更为擅长。 |  
| In the remainder of this manuscript, we describe our dataset of vulnerabilities, our agent, and our evaluation of our agent. | 在本文的其余部分，我们将描述我们的漏洞数据集、我们的代理以及我们对代理的评估。 |

## Content Summary

以下是《The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies》论文的详细总结：

### 1. 论文内容概述
本文重点探讨了大型语言模型（LLM）代理在网络安全领域中的潜在威胁和利用方式。随着LLM的能力不断提升，其在网络安全中的应用也逐渐受到关注。论文特别研究了LLM代理自主利用“一日漏洞”（one-day vulnerabilities）的能力，这类漏洞指的是已被披露但尚未修补的系统漏洞。通过实验，研究表明，GPT-4代理能够在给定漏洞描述的情况下，以高达87%的成功率自动利用这些漏洞，而其他模型和开源漏洞扫描工具的成功率为0%。

### 2. 具体研究内容
论文的研究内容可以分为以下几个部分：

- **背景和动机**：LLM代理的能力不断增强，引发了其在网络安全领域潜在用途的关注。过去的研究主要集中在LLM辅助人类进行安全操作，或在简单的“夺旗竞赛”环境下进行测试，而这些环境并不反映真实的网络安全挑战。本文旨在探索LLM代理在真实世界中的自主攻击能力。

- **漏洞基准测试**：研究者收集了15个真实世界的“一日漏洞”，这些漏洞包括真实网站、容器管理软件以及易受攻击的Python包。漏洞的严重性范围从中等到极高，超过一半的漏洞在GPT-4的知识截止日期之后被披露。这些漏洞为本文提供了一个真实且具有挑战性的测试环境。

- **LLM代理的设计**：研究者设计了一个基于GPT-4的LLM代理，结合ReAct代理框架，并为其提供工具使用能力。该代理能够读取并利用CVE描述，执行一系列操作来利用漏洞。实验显示，GPT-4是唯一能够在这些漏洞上取得成功的模型，而其他模型在这些测试中均未能成功。

### 3. 实验分析
实验部分集中于评估LLM代理在不同漏洞利用任务中的表现：

- **成功率评估**：GPT-4在给定CVE描述的情况下，能够以87%的成功率利用漏洞。而当去掉CVE描述后，成功率急剧下降至7%，表明发现漏洞的难度远高于利用漏洞。

- **模型比较**：GPT-4显著优于其他LLM模型和开源工具（如ZAP和Metasploit），后者未能在任何漏洞上取得成功。这表明GPT-4在自主利用网络漏洞方面展现了突出的能力。

- **成本分析**：实验还评估了使用GPT-4进行漏洞利用的成本，平均每次攻击的成本约为3.52美元，相比人工成本显著降低。

### 4. 结论
本文得出结论，LLM代理特别是GPT-4，已经展现出自主利用真实世界网络漏洞的能力。这一发现引发了对LLM代理在网络安全中应用的进一步思考，并强调了在部署这些强大工具时所需的审慎态度。研究建议进一步探索LLM代理的规划和子代理功能，以提高其漏洞利用的成功率。

以上总结包含了论文的核心研究内容、实验分析和结论，提供了对LLM代理在网络安全领域的能力及其可能带来的威胁的全面理解。