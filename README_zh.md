# 最全面的大模型安全论文

---

![Academic Alpaca](resources/DALL·E%202024-07-30%2015.10.44%20-%20An%20academic-looking%20alpaca%20wearing%20scholarly%20glasses%20and%20a%20graduation%20cap%2C%20with%20an%20intellectual%20and%20serious%20expression.%20The%20background%20should%20be%20a%20lib.webp)

🎉🎉🎉 **欢迎来到LLM安全研究资源库，这是您获取最新、最全面的LLM安全研究的首选目的地！**

随着人工智能领域的迅速发展，伴随大模型的安全挑战也在不断增加。我们的资源库处于这一新兴学科的前沿，提供全面且不断更新的研究论文。我们的资源库具有以下几个关键优势：

- 🔥 **前沿的安全研究**：专注于最新和最具创新性的针对大语言模型的安全研究，确保您在这一关键领域保持领先。

- ⏰️ **实时更新**：我们的资源库实时更新，提供最及时、最相关的研究成果。

- 📚️ **全面覆盖**：我们旨在涵盖大模型安全的各个方面，从理论基础到实际应用，我们的收藏将不断扩展，以包括这一重要领域的每一个方面。

- 🇨🇳 **高质量的双语内容**：我们提供高质量的中文翻译，便于快速阅读。

- 🌟 **大模型总结**：领域专家借助 ChatGPT4 Agent 为论文做详细总结，便于您快速直观地了解论文的研究内容和实验逻辑。

加入我们，探索AI安全的前沿，为大模型的更安全未来做出贡献。今天就深入我们广泛且不断增长的研究论文库，在快速发展的AI模型安全领域保持领先！！！

## 提示注入

| 论文标题 | 日期 | 发布 | 标签 |
|---------|-----|------|------|
| [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models  受污染的RAG：知识中毒攻击在检索增强的大语言模型生成中](paper_list/PoisonedRAG_Knowledge_Poisoning_Attacks_to_Retrieval-Augmented_Generation_of_Large_Language_Models.md) | 2024.2.12 | arXiv | 攻击 |

## 数据库污染
| 论文标题 | 日期 | 发布 | 标签 |
|---------|-----|------|------|
| [Poisoning Retrieval Corpora by Injecting Adversarial Passages 通过注入对抗性段落污染检索语料库](paper_list/Poisoning_Retrieval_Corpora_by_Injecting_Adversarial_Passages.md) | 2023.10.29 | arXiv | 攻击 |
| [Corpus Poisoning via Approximate Greedy Gradient Descent 通过近似贪婪梯度下降污染语料库](paper_list/Corpus_Poisoning_via_Approximate_Greedy_Gradient_Descent.md) | 2024.6.7 | arXiv | 攻击 |

## 越狱

| 论文标题 | 日期 | 发布 | 标签 |
|---------|-----|------|------|
| [Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models 不要听我讲：理解和探索大型语言模型的越狱提示](paper_list/Don't_Listen_To_Me:_Understanding_and_Exploring_Jailbreak_Prompts_of_Large_Language_Models.md) | 2023.3.26 | USENIX Security 2024 | 实验 攻击 |
| [Many shot Jailbreaking 多次越狱](paper_list/Many_shot_Jailbreaking.md) | 2024.4.2 | USENIX Security 2024 | 方法 攻击 |

## 数据提取与隐私

| 论文标题 | 日期 | 发布 | 标签 |
|---------|-----|------|------|
| [Bag of Tricks for Training Data Extraction from Language Models 从语言模型中提取训练数据的技巧集锦](paper_list/Bag_of_Tricks_for_Training_Data_Extraction_from_Language_Models.md) | 2023.2.9 | arXiv | 实验 攻击 |

## 代理

| 论文标题 | 日期 | 发布 | 标签 |
|---------|-----|------|------|
| [The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies 大模型代理的新兴安全与隐私：案例研究综述](paper_list/The_Emerged_Security_and_Privacy_of_LLM_Agent_A_Survey_with_Case_Studies.md) | 2024.7.28 | arXiv | 综述 攻击 防御 |
| [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents BadAgent：在LLM代理中插入和激活后门攻击](paper_list/Bad_Agent_Inserting_and_Activating_Backdoor_Attacks_in_LLM_Agents.md) | 2024.6.5 | arXiv | 攻击 |
| [INJECAGENT: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents INJECAGENT：工具集成大型语言模型代理中间接提示注入的基准](paper_list/INJECAGENT_Benchmarking_Indirect_Prompt_Injections_in_Tool-Integrated_Large_Language_Model_Agents.md) | 2024.3.24 | arXiv | 攻击 提示注入 |
| [LLM Agents can Autonomously Exploit One-day Vulnerabilities LLM代理可以自主利用一天漏洞](paper_list/LLM_Agents_can_Autonomously_Exploit_One-day_Vulnerabilities.md) | 2023.5.22 | arXiv | 应用 攻击 |

## 模型特性

| 论文标题 | 日期 | 发布 | 标签 |
|---------|-----|------|------|
| [Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts 在现实知识冲突下研究大型语言模型行为](paper_list/Studying_Large_Language_Model_Behaviors_Under_Realistic_Knowledge_Conflicts.md) | 2024.4.24 | arXiv | 实验 RAG |
| [Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS 自适应变色龙还是固执的树懒：揭示大型语言模型在知识冲突中的行为](paper_list/Adaptive_Chameleon_or_Stubborn_Sloth_Revealing_the_Behavior_of_Large_Language_Models_in_Knowledge_Conflicts.md) | 2024.7.28 | arXiv | 实验 RAG |

## 贡献
欢迎您对本仓库的贡献！

如果您对本论文列表有任何疑问，或者寻求LLM安全领域的研究合作，欢迎通过 yaojialzc@gmail.com 与我联系。
